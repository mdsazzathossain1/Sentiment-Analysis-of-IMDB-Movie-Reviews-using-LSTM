# Sentiment-Analysis-of-IMDB-Movie-Reviews-using-LSTM
 Project Overview
The goal of this notebook is to classify movie reviews as either "positive" or "negative". The project starts by exploring the powerful text embeddings generated by a pre-trained BERT model and visualizes them to understand their structure. Following this exploration, it builds a classic Recurrent Neural Network (RNN) using an LSTM layer to perform the final classification task.\
✨ Key Features\
Data Loading: Fetches the 50k IMDB Movie Review dataset using the Hugging Face datasets library.\
Text Preprocessing: Utilizes the BertTokenizer for efficient and robust text tokenization.\
Embedding Visualization: Employs dimensionality reduction techniques like PCA and t-SNE to visualize high-dimensional BERT token embeddings in 2D space.\
Model Architecture: Implements a custom sentiment analysis model in PyTorch consisting of an Embedding layer, an LSTM layer, and a Linear layer.\
Model Training & Evaluation: Includes a complete training loop with an Adam optimizer and Cross-Entropy loss function, followed by a thorough evaluation on the test set.\
Inference: Demonstrates how to save the trained model and tokenizer, and then load them to perform sentiment prediction on new, unseen text.\
⚙️ How It Works\
The notebook is structured to guide you through the process of building a sentiment classifier step-by-step.\
Environment Setup: The initial cells install necessary libraries like opendatasets, datasets, and torch.\
Data Loading: The IMDB dataset, containing 50,000 movie reviews, is loaded into memory using load_dataset('imdb').\
Tokenization: The text reviews are converted into numerical tokens using the pre-trained BertTokenizer. This step also applies padding and truncation to ensure all sequences have the same length (128 tokens).\
Embedding Extraction & Visualization: To understand the text data, embeddings for a sample review are generated using a pre-trained BERT model. These high-dimensional embeddings are then reduced to two dimensions\ using PCA and t-SNE and plotted to visually inspect the relationships between tokens.\
Data Preparation for PyTorch: The tokenized dataset is converted into PyTorch TensorDataset and then wrapped in DataLoader objects to efficiently feed data to the model in batches.\
LSTM Model Definition: A custom SentimentModel class is created. This model learns to map the sequence of token embeddings to a final sentiment prediction.\
Training the Model: The LSTM model is trained on the training dataset for 10 epochs. The script prints the loss and accuracy at the end of each epoch, showing the model's learning progress.\
Evaluating the Model: After training, the model's performance is measured on the unseen test dataset to get a final accuracy score.\
Saving and Loading: The trained model weights and the tokenizer are saved to disk. The notebook then shows how to load these files back for inference.\
Making Predictions: In the final step, a new sample review is fed to the loaded model to predict its sentiment, demonstrating the practical application of the trained classifier.
